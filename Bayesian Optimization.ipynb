# installing library for Bayesian optimization
!pip install hyperopt

X = X_data
y = Y_data

ET = ExtraTreesClassifier()

batch_sz_list = [8,16,32,64,128,256]

# Defining the hyper parameter space as a dictionary
parameter_space = { 'num_trees': hp.quniform('num_trees',5,50,1),
                    'depth': hp.quniform('depth',3,50,1),
                    'used_features_rate': hp.quniform('used_features_rate',0.5,1,.1),
                    'batch_size': hp.choice('batch_size',batch_sz_list),
                    'num_epochs': hp.quniform('num_epochs', 10,50,1)
                  }

# Defining a cost function which the Bayesian algorithm will optimize
def objective(parameter_space):
    Error = cross_val_score(ET, X, y, cv = 10, scoring='accuracy').mean()

    # We return the loss which will be minimized by the fmin() function
    return {'loss': -Error, 'status': STATUS_OK }

# Finding out which set of hyperparameters give highest accuracy
trials = Trials()
best_params = fmin(fn= objective,
            space= parameter_space,
            #algo= tpe.suggest,
            algo=anneal.suggest,  # the logic which chooses next parameter to try
            max_evals = 100,
            trials= trials)

print('The best parameters are:', best_params)

learning_rate = 0.01
num_trees = int(best_params['num_trees'])
depth = int(best_params['depth'])
used_features_rate = round(best_params['used_features_rate'],1)
batch_size = int(batch_sz_list[best_params['batch_size']])
num_epochs = int(best_params['num_epochs'])

print(f"num_trees : {num_trees} depth : {depth}  used_features_rate : {used_features_rate}")
print(f"learning_rate : {learning_rate}  batch_size : {batch_size}  num_epochs : {num_epochs}")