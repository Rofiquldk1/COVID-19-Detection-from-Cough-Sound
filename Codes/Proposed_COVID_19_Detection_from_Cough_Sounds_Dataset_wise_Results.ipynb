{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZOQ1hIzRCSL"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn==1.3.2 --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVAgbYQvy7k6"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, recall_score, precision_score\n",
        "from imblearn.metrics import specificity_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from itertools import product\n",
        "from statistics import mean, stdev\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, anneal\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry2j8XKNRV7E"
      },
      "source": [
        "Declare the necessary variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jukR31PIGex"
      },
      "outputs": [],
      "source": [
        "# declare the necessary variables\n",
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "all_data_file = \"all_data.csv\"\n",
        "X_data_file = \"X_data.csv\"\n",
        "Y_data_file = \"Y_data.csv\"\n",
        "TARGET_LABELS = [0.0,1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ccpLxeRcQS"
      },
      "source": [
        "Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8pCqkYZILhA"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# replace with your Google Drive link in np.load()\n",
        "cambridgeAsymptomaticdatasetX = np.load('/content/drive/MyDrive/data/Cambridge/Task 1/cough_X_features_np.npy')\n",
        "cambridgeAsymptomaticdatasetY = np.load('/content/drive/MyDrive/data/Cambridge/Task 1/cough_y_features_np.npy')\n",
        "\n",
        "cambridgeSymptomaticdatasetX = np.load('/content/drive/MyDrive/data/Cambridge/Task 2/cough_X_features_np.npy')\n",
        "cambridgeSymptomaticdatasetY = np.load('/content/drive/MyDrive/data/Cambridge/Task 2/cough_y_features_np.npy')\n",
        "\n",
        "coswaradatasetX = np.load('/content/drive/MyDrive/data/Coswara/cough_X_features_np.npy')\n",
        "coswaradatasetY = np.load('/content/drive/MyDrive/data/Coswara/cough_y_features_np.npy')\n",
        "\n",
        "coughviddatasetX = np.load('/content/drive/MyDrive/data/COUGHVID/cough_X_features_np.npy')\n",
        "coughviddatasetY = np.load('/content/drive/MyDrive/data/COUGHVID/cough_y_features_np.npy')\n",
        "\n",
        "virufydatasetX = np.load('/content/drive/MyDrive/data/Virufy/cough_X_features_np.npy')\n",
        "virufydatasetY = np.load('/content/drive/MyDrive/data/Virufy/cough_y_features_np.npy')\n",
        "\n",
        "virufyNoCoCoDadatasetX = np.load('/content/drive/MyDrive/data/NOCOCODA and Virufy/cough_X_features_np.npy')\n",
        "virufyNoCoCoDadatasetY = np.load('/content/drive/MyDrive/data/NOCOCODA and Virufy/cough_y_features_np.npy')\n",
        "\n",
        "ComPareX = np.load('/content/drive/MyDrive/data/ComPare/cough_X_features_np.npy')\n",
        "ComPareY = np.load('/content/drive/MyDrive/data/ComPare/cough_y_features_np.npy')\n",
        "\n",
        "\n",
        "# cambridgeAsymptomaticdataset,cambridgeSymptomaticdataset,coswaradataset,coughviddataset,virufydataset,virufyNoCoCoDadataset\n",
        "datasetX = virufydatasetX # Replace dataset name + 'X'\n",
        "datasetY = virufydatasetY # Replace dataset name + 'Y'\n",
        "\n",
        "train_index = []\n",
        "test_index = []\n",
        "\n",
        "# split train and test data\n",
        "for i in range(10):\n",
        "   # replace dataset name (cambridgeAsymptomaticdataset,cambridgeSymptomaticdataset,coswaradataset,coughviddataset,virufydataset,virufyNoCoCoDadataset)\n",
        "   train_index_url = '/content/drive/MyDrive/covid-19 cough sound dataset/virufydataset/train/'+str(i)+'.csv'\n",
        "   test_index_url = '/content/drive/MyDrive/covid-19 cough sound dataset/virufydataset/test/'+str(i)+'.csv'\n",
        "\n",
        "   df = pd.read_csv(train_index_url)\n",
        "   label=df['train_index'].tolist()\n",
        "   train_array=np.array(label)\n",
        "   train_index.append(train_array)\n",
        "\n",
        "   df = pd.read_csv(test_index_url)\n",
        "   label=df['test_index'].tolist()\n",
        "   test_array=np.array(label)\n",
        "   test_index.append(test_array)\n",
        "\n",
        "\n",
        "print(f\"cambridgeAsymptomaticdatasetX shape : {cambridgeAsymptomaticdatasetX.shape}\")\n",
        "print(f\"cambridgeAsymptomaticdatasetY shape : {cambridgeAsymptomaticdatasetY.shape}\")\n",
        "print('')\n",
        "\n",
        "print(f\"cambridgeSymptomaticdatasetX shape : {cambridgeSymptomaticdatasetX.shape}\")\n",
        "print(f\"cambridgeSymptomaticdatasetY shape : {cambridgeSymptomaticdatasetY.shape}\")\n",
        "print('')\n",
        "\n",
        "\n",
        "print(f\"coswaradatasetX shape : {coswaradatasetX.shape}\")\n",
        "print(f\"coswaradatasetY shape : {coswaradatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"coughviddatasetX shape : {coughviddatasetX.shape}\")\n",
        "print(f\"coughviddatasetY shape : {coughviddatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"virufydatasetX shape : {virufydatasetX.shape}\")\n",
        "print(f\"virufydatasetY shape : {virufydatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"virufyNoCoCoDadatasetX shape : {virufyNoCoCoDadatasetX.shape}\")\n",
        "print(f\"virufyNoCoCoDadatasetY shape : {virufyNoCoCoDadatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"ComPareX shape : {ComPareX.shape}\")\n",
        "print(f\"ComPareY shape : {ComPareY.shape}\")\n",
        "\n",
        "\n",
        "X_data_ls = []\n",
        "Y_data_ls = []\n",
        "\n",
        "rows,columns = datasetX.shape\n",
        "\n",
        "C=0\n",
        "N=0\n",
        "\n",
        "for i in range(rows):\n",
        "    col = []\n",
        "    arr = datasetX[i]\n",
        "    for j in range(1,len(arr)+1,1):\n",
        "       col.append(float(arr[j-1]))\n",
        "    X_data_ls.append(col)\n",
        "    if(datasetY[i]=='C'):\n",
        "       Y_data_ls.append(0)\n",
        "       C+=1\n",
        "    else:\n",
        "       Y_data_ls.append(1)\n",
        "       N+=1\n",
        "\n",
        "X_data = np.array(X_data_ls)\n",
        "Y_data = np.array(Y_data_ls)\n",
        "\n",
        "print(f\"X_data: {X_data.shape}\")\n",
        "print(f\"Y_data: {Y_data.shape}\")\n",
        "\n",
        "print(f\"C: {C}\")\n",
        "print(f\"N: {N}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJGLergmRimP"
      },
      "source": [
        "RFECV+Extra-trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaUFZOAxIVun"
      },
      "outputs": [],
      "source": [
        "min_features_to_select = 1  # Minimum number of features to consider\n",
        "\n",
        "# feature selection\n",
        "#def select_features(X_train, y_train, X_test):\n",
        "def select_features(X_train,y_train,X_data):\n",
        "  # Create the RFE object and compute a cross-validated score.\n",
        "  #fs_model = RandomForestClassifier(n_estimators = 800,random_state = 0)\n",
        "  fs_model = ExtraTreesClassifier(n_estimators=50,random_state = 0)\n",
        "  #fs_model = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
        "  rfecv = RFECV(estimator=fs_model, step=1, cv=StratifiedKFold(10),scoring='roc_auc',min_features_to_select=min_features_to_select)\n",
        "\n",
        "  print(f\"shape of X_data before transform: {X_data.shape}\")\n",
        "\n",
        "  rfecv.fit(X_train, y_train)\n",
        "\n",
        "  # transform train input data\n",
        "  X_data_fs = rfecv.transform(X_data)\n",
        "  print(f\"shape of X_data_fs after transform: {X_data_fs.shape}\")\n",
        "\n",
        "  return X_data_fs, rfecv\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data,Y_data,test_size=0.20, random_state=42)\n",
        "X_data_fs, rfecv = select_features(X_train,y_train,X_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU6MW0_7Icr8"
      },
      "outputs": [],
      "source": [
        "rows,columns = X_data_fs.shape\n",
        "\n",
        "TARGET_FEATURE_NAME = str(columns+1)\n",
        "#Derive the necessary variables\n",
        "#No. of classes, assuming last column is dependent variable\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "#print([x for x in range(5)])\n",
        "#List of all the columns\n",
        "CSV_HEADER = []\n",
        "for i in range(columns+1):\n",
        "    CSV_HEADER.append(str(i+1))\n",
        "\n",
        "# A list of the numerical feature names.\n",
        "#List of only independent variables\n",
        "FEATURE_NAMES = []\n",
        "\n",
        "for i in range(columns):\n",
        "    FEATURE_NAMES.append(str(i+1))\n",
        "\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in CSV_HEADER  else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scxwsy6KRp3Y"
      },
      "source": [
        "Bayesian optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iME1kPn4Iiu3"
      },
      "outputs": [],
      "source": [
        "print(f\"X_data: {X_data.shape}\")\n",
        "print(f\"Y_data: {Y_data.shape}\")\n",
        "\n",
        "\"\"\"\n",
        "# You can try hyperparameter tuning with Bayesian Optimization to gain hands-on experience.\n",
        "\n",
        "# installing library for Bayesian optimization\n",
        "!pip install hyperopt\n",
        "\n",
        "X = X_data\n",
        "y = Y_data\n",
        "\n",
        "\n",
        "##############################################################\n",
        "# Bayesian hyperparameter optimization\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "ET = ExtraTreesClassifier()\n",
        "\n",
        "batch_sz_list = [8,16,32,64,128,256]\n",
        "\n",
        "# Defining the hyper parameter space as a dictionary\n",
        "parameter_space = { 'num_trees': hp.quniform('num_trees',5,50,1),\n",
        "                    'depth': hp.quniform('depth',3,50,1),\n",
        "                    'used_features_rate': hp.quniform('used_features_rate',0.5,1,.1),\n",
        "                    'batch_size': hp.choice('batch_size',batch_sz_list),\n",
        "                    'num_epochs': hp.quniform('num_epochs', 10,50,1)\n",
        "                  }\n",
        "\n",
        "# Defining a cost function which the Bayesian algorithm will optimize\n",
        "def objective(parameter_space):\n",
        "\n",
        "    # The accuracy parameter is the average accuracy obtained by cross validation of the data\n",
        "    # See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "    Error = cross_val_score(ET, X, y, cv = 10, scoring='accuracy').mean()\n",
        "\n",
        "    # We return the loss which will be minimized by the fmin() function\n",
        "    return {'loss': -Error, 'status': STATUS_OK }\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Finding out which set of hyperparameters give highest accuracy\n",
        "trials = Trials()\n",
        "best_params = fmin(fn= objective,\n",
        "            space= parameter_space,\n",
        "            #algo= tpe.suggest,\n",
        "            algo=anneal.suggest,  # the logic which chooses next parameter to try\n",
        "            max_evals = 100,\n",
        "            trials= trials)\n",
        "#The best hyperparameters are returned by the function ‘fmin()’.\n",
        "#We have stored the results in the ‘best_params’ variable.\n",
        "print('The best parameters are:', best_params)\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_trees = int(best_params['num_trees'])\n",
        "depth = int(best_params['depth'])\n",
        "used_features_rate = round(best_params['used_features_rate'],1)\n",
        "batch_size = int(batch_sz_list[best_params['batch_size']])\n",
        "num_epochs = int(best_params['num_epochs'])\n",
        "\n",
        "print(f\"num_trees : {num_trees} depth : {depth}  used_features_rate : {used_features_rate}\")\n",
        "print(f\"learning_rate : {learning_rate}  batch_size : {batch_size}  num_epochs : {num_epochs}\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# To save time on Bayesian Optimization, use the hyperparameters below, which have already been calculated for optimal performance.\n",
        "\n",
        "#for strategy1 && strategy2 && strategy3 (Without hyperparameter tuning via Bayesian Optimization)\n",
        "num_trees = 10\n",
        "depth = 10\n",
        "used_features_rate = 1.0\n",
        "learning_rate = 0.01\n",
        "batch_size = 256\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "#for strategy4 && strategy5 (Bayesian Optimization)\n",
        "#cambridge_asymptomatic_eT\n",
        "num_trees = 18\n",
        "depth = 8\n",
        "used_features_rate = 0.8\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "num_epochs = 17\n",
        "\n",
        "#cambridge_symptomatic_eT\n",
        "num_trees = 25\n",
        "depth = 9\n",
        "used_features_rate = 0.8\n",
        "learning_rate = 0.01\n",
        "batch_size = 8\n",
        "num_epochs = 13\n",
        "\n",
        "#coswara_eT\n",
        "num_trees = 25\n",
        "depth = 11\n",
        "used_features_rate = 0.6\n",
        "learning_rate = 0.01\n",
        "batch_size = 16\n",
        "num_epochs = 14\n",
        "\n",
        "#coughvid_eT\n",
        "num_trees = 17\n",
        "depth = 6\n",
        "used_features_rate = 1.0\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "num_epochs = 19\n",
        "\n",
        "#virufy_eT\n",
        "num_trees = 16\n",
        "depth = 16\n",
        "used_features_rate = 0.8\n",
        "learning_rate = 0.01\n",
        "batch_size = 8\n",
        "num_epochs = 22\n",
        "\n",
        "#Virufy + NoCoCoDa_eT\n",
        "num_trees = 29\n",
        "depth = 5\n",
        "used_features_rate = 0.6\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "num_epochs = 34\n",
        "\"\"\"\n",
        "\n",
        "num_trees = 16\n",
        "depth = 16\n",
        "used_features_rate = 0.8\n",
        "learning_rate = 0.01\n",
        "batch_size = 8\n",
        "num_epochs = 22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_uzqQFIjz6"
      },
      "outputs": [],
      "source": [
        "# import the data from csv with all the relevant columns\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128, num_epochs=1):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=num_epochs,\n",
        "        #num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(lambda features, target: (features, target))\n",
        "    return dataset.cache()\n",
        "\n",
        "# create one input layer for each feature\n",
        "def create_model_inputs(FEATURE_NAMES):\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = layers.Input(\n",
        "            name=feature_name, shape=(), dtype=tf.float32)\n",
        "    return inputs\n",
        "\n",
        "# If there is any categorical feature, it can be encoded\n",
        "# Since we had only numerical feature, we will skip this step\n",
        "# We will also exdpand the dimension of the feature inputs\n",
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        encoded_feature = inputs[feature_name]\n",
        "        if inputs[feature_name].shape[-1] is None:\n",
        "            encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OLVLMRFIoGy"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionTree, self).__init__()\n",
        "        self.depth = depth   # Pre-defined depth\n",
        "        self.num_leaves = 2 ** depth  # No of leaves in the tree\n",
        "        self.num_classes = num_classes  # No of classes in the dependent variable\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        # Number of features to be selected for each tree\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        # Select \"num_used_features\" features from the total features\n",
        "        one_hot = np.eye(num_features)\n",
        "        \"\"\"\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        \"\"\"\n",
        "        try:\n",
        "             sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=False)\n",
        "        except ValueError:\n",
        "             sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=True)\n",
        "\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )\n",
        "        # Initiate mu, the probablity of a sample reaching a leaf node\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        # Update probabilities in each level and node.\n",
        "        # Calculate total final output probability\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y8FJDlDIssj"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionForest, self).__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZIXSkg2IxCR"
      },
      "outputs": [],
      "source": [
        "def create_tree_model():\n",
        "    inputs = create_model_inputs(FEATURE_NAMES)\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP_kAqKKI2GH"
      },
      "outputs": [],
      "source": [
        "# create the forest model by taking input, output and the model structure\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs(FEATURE_NAMES)\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "725tgC2KR_CG"
      },
      "source": [
        "SMOTE and Threshold moving (ROC-AUC score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCN4v3JOJAOc"
      },
      "outputs": [],
      "source": [
        "from numpy import arange\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "\n",
        "\"\"\"\n",
        "On a binary classification problem with class labels 0 and 1, normalized predicted probabilities and a threshold of 0.5, then values less than the\n",
        "threshold of 0.5 are assigned to class 0 and values greater than or equal to 0.5 are assigned to class 1.\n",
        "\n",
        "Prediction < 0.5 = Class 0\n",
        "Prediction >= 0.5 = Class 1\n",
        "\"\"\"\n",
        "\n",
        "# define thresholds\n",
        "thresholds = arange(0, 1, 0.001)\n",
        "\n",
        "# apply threshold to positive probabilities to create labels\n",
        "def to_labels(pos_probs, threshold):\n",
        "\treturn (pos_probs >= threshold).astype('int')\n",
        "\n",
        "# now compile the model, train it on train sample and predict it for test sample.\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "\n",
        "    lst_roc_auc = []\n",
        "    lst_precision = []\n",
        "    lst_recall = []\n",
        "    lst_f1 = []\n",
        "    lst_acc = []\n",
        "    lst_speci = []\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    for i in range(10):\n",
        "        X_train, X_test = X_data_fs[train_index[i]], X_data_fs[test_index[i]]\n",
        "        y_train, y_test = Y_data[train_index[i]], Y_data[test_index[i]]\n",
        "\n",
        "        print('Before Oversampling')\n",
        "        print(f'X_train shape: {X_train.shape}')\n",
        "        print(f'y_train shape: {y_train.shape}')\n",
        "        counter=Counter(y_train)\n",
        "        print(counter)\n",
        "        oversample = SVMSMOTE()\n",
        "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "        print('After Oversampling')\n",
        "        print(f'X_train shape: {X_train.shape}')\n",
        "        print(f'y_train shape: {y_train.shape}')\n",
        "        counter = Counter(y_train)\n",
        "        print(counter)\n",
        "\n",
        "        all_data_train_fold = []\n",
        "        rows,columns = X_train.shape\n",
        "\n",
        "        for i in range(rows):\n",
        "           col2 = []\n",
        "           arr = X_train[i]\n",
        "           for j in range(1,len(arr)+1,1):\n",
        "              col2.append(float(arr[j-1]))\n",
        "           col2.append(y_train[i])\n",
        "           all_data_train_fold.append(col2);\n",
        "\n",
        "        print(f'all_data_train_fold shape: {np.array(all_data_train_fold).shape}')\n",
        "\n",
        "        all_data_test_fold = []\n",
        "        rows,columns = X_test.shape\n",
        "\n",
        "        for i in range(rows):\n",
        "           col2 = []\n",
        "           arr = X_test[i]\n",
        "           for j in range(1,len(arr)+1,1):\n",
        "              col2.append(float(arr[j-1]))\n",
        "           col2.append(y_test[i])\n",
        "           all_data_test_fold.append(col2);\n",
        "\n",
        "        print(f'all_data_test_fold shape: {np.array(all_data_test_fold).shape}')\n",
        "\n",
        "        # convert array into dataframe\n",
        "        DF = pd.DataFrame(all_data_train_fold)\n",
        "        # save the dataframe as a csv file\n",
        "        DF.to_csv(train_data_file, index=False, header=False)\n",
        "        train_data = get_dataset_from_csv(\n",
        "           train_data_file, batch_size=batch_size, num_epochs=num_epochs\n",
        "        )\n",
        "\n",
        "        model.fit(train_data)\n",
        "\n",
        "        # convert array into dataframe\n",
        "        DF1 = pd.DataFrame(all_data_test_fold)\n",
        "        # save the dataframe as a csv file\n",
        "        DF1.to_csv(test_data_file, index=False, header=False)\n",
        "        test_data = get_dataset_from_csv(\n",
        "           test_data_file, batch_size=batch_size\n",
        "        )\n",
        "        #_, accuracy = model.evaluate(test_data)\n",
        "        #lst_accu_stratified.append(round(accuracy, 2))\n",
        "\n",
        "        #print(f\"y_test: {y_test}\")\n",
        "        true_categories = tf.concat([y for x, y in test_data], axis=0)\n",
        "        print(f\"true_categories: {true_categories}\")\n",
        "\n",
        "        # keep probabilities for the positive outcome only\n",
        "        Y_pred = model.predict(test_data)\n",
        "        #print(Y_pred)\n",
        "        probs = Y_pred[:, 1]\n",
        "        #print(f\"probs: {probs}\")\n",
        "\n",
        "        # evaluate each threshold\n",
        "        #scores = [precision_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        #scores = [f1_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        # get best threshold\n",
        "        ix = argmax(scores)\n",
        "        #print(f\"Optimal threshold score: {ix}\")\n",
        "        #print(f\"Thresholds[ix]: {thresholds[ix]}\")\n",
        "\n",
        "        #Y_pred = model.predict(test_data)\n",
        "        #probs = Y_pred[:, 1]\n",
        "        #print(probs)\n",
        "        #print(f\"probsLast: {probs}\")\n",
        "        #print(f\"to_labels(probs, thresholds[ix]): {to_labels(probs[len(probs)-1], thresholds[ix])}\")\n",
        "        print(f\"TM: {thresholds[ix]}\")\n",
        "        print(f\"To Labels(probs, thresholds[ix]): {to_labels(probs, thresholds[ix])}\")\n",
        "\n",
        "        conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))\n",
        "        print(conf_matrix)\n",
        "\n",
        "        tp, fn, fp, tn = conf_matrix.ravel()\n",
        "        print(f\"True positive: {tp}\")\n",
        "        print(f\"True negative: {tn}\")\n",
        "        print(f\"False positive: {fp}\")\n",
        "        print(f\"False negative: {fn}\")\n",
        "\n",
        "        lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));\n",
        "\n",
        "        if(tp+fp == 0):\n",
        "           lst_precision.append(0)\n",
        "        else:\n",
        "           lst_precision.append(tp/(tp+fp));\n",
        "\n",
        "        if(tp+fn == 0):\n",
        "           lst_recall.append(0)\n",
        "        else:\n",
        "           lst_recall.append(tp/(tp+fn));\n",
        "\n",
        "        print(f\"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}\")\n",
        "        #print(f\"precision : {precision_score(true_categories,to_labels(probs, thresholds[ix]))}\")\n",
        "        #print(f\"recall_score : {recall_score(true_categories,to_labels(probs, thresholds[ix]))}\")\n",
        "\n",
        "        print('')\n",
        "\n",
        "    print(f\"mean_of_roc_auc_score : {mean(lst_roc_auc)}\")\n",
        "    print(f\"mean_of_precision : {mean(lst_precision)}\")\n",
        "    print(f\"mean_of_recall : {mean(lst_recall)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY08NEEq1FYy"
      },
      "outputs": [],
      "source": [
        "class ExpandDimsLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.expand_dims(inputs, -1)\n",
        "\n",
        "# If there is any categorical feature, it can be encoded\n",
        "# Since we had only numerical feature, we will skip this step\n",
        "# We will also exdpand the dimension of the feature inputs\n",
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        encoded_feature = inputs[feature_name]\n",
        "        if inputs[feature_name].shape[-1] is None:\n",
        "            encoded_feature = ExpandDimsLayer()(encoded_feature)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvm95gJSTuN"
      },
      "source": [
        "DNDT model creation and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxfA5gElJFaH"
      },
      "outputs": [],
      "source": [
        "tree_model = create_tree_model()\n",
        "run_experiment(tree_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtxos2PXSbuU"
      },
      "source": [
        "DNDF model creation and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LORicLyBJKyR"
      },
      "outputs": [],
      "source": [
        "forest_model = create_forest_model()\n",
        "run_experiment(forest_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
