{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn==1.3.2 --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gDh9z8x3_TR8"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, recall_score, precision_score\n",
        "from imblearn.metrics import specificity_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from itertools import product\n",
        "from statistics import mean, stdev\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhJAOI72M7BD"
      },
      "outputs": [],
      "source": [
        "# declare the necessary variables\n",
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "all_data_file = \"all_data.csv\"\n",
        "X_data_file = \"X_data.csv\"\n",
        "Y_data_file = \"Y_data.csv\"\n",
        "TARGET_LABELS = [0.0,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pASFbVE7NJpI",
        "outputId": "c815a220-ee35-4b2a-ba95-532b9c2c0c98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cambridgeAsymptomaticdatasetX shape : (439, 193)\n",
            "cambridgeAsymptomaticdatasetY shape : (439,)\n",
            "cambridgeSymptomaticdatasetX shape : (86, 193)\n",
            "cambridgeSymptomaticdatasetY shape : (86,)\n",
            "coswaradatasetX shape : (1319, 193)\n",
            "coswaradatasetY shape : (1319,)\n",
            "coughviddatasetX shape : (1360, 193)\n",
            "coughviddatasetY shape : (1360,)\n",
            "virufydatasetX shape : (121, 193)\n",
            "virufydatasetY shape : (121,)\n",
            "virufyNoCoCoDadatasetX shape : (194, 193)\n",
            "virufyNoCoCoDadatasetY shape : (194,)\n",
            "\n",
            "X_data: (3398, 193)\n",
            "Y_data: (3398,)\n",
            "data_all: (3398, 194)\n",
            "\n",
            "all_data: (439, 194)\n",
            "all_data: (86, 194)\n",
            "all_data: (1319, 194)\n",
            "all_data: (1360, 194)\n",
            "all_data: (194, 194)\n",
            "datasetX: (439, 193)\n",
            "datasetX: (86, 193)\n",
            "datasetX: (1319, 193)\n",
            "datasetX: (1360, 193)\n",
            "datasetX: (194, 193)\n",
            "datasetY: (439,)\n",
            "datasetY: (86,)\n",
            "datasetY: (1319,)\n",
            "datasetY: (1360,)\n",
            "datasetY: (194,)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# replace with your Google Drive link in np.load()\n",
        "cambridgeAsymptomaticdatasetX = np.load('/content/drive/MyDrive/data/Cambridge/Task 1/cough_X_features_np.npy')\n",
        "cambridgeAsymptomaticdatasetY = np.load('/content/drive/MyDrive/data/Cambridge/Task 1/cough_y_features_np.npy')\n",
        "\n",
        "cambridgeSymptomaticdatasetX = np.load('/content/drive/MyDrive/data/Cambridge/Task 2/cough_X_features_np.npy')\n",
        "cambridgeSymptomaticdatasetY = np.load('/content/drive/MyDrive/data/Cambridge/Task 2/cough_y_features_np.npy')\n",
        "\n",
        "coswaradatasetX = np.load('/content/drive/MyDrive/data/Coswara/cough_X_features_np.npy')\n",
        "coswaradatasetY = np.load('/content/drive/MyDrive/data/Coswara/cough_y_features_np.npy')\n",
        "\n",
        "coughviddatasetX = np.load('/content/drive/MyDrive/data/COUGHVID/cough_X_features_np.npy')\n",
        "coughviddatasetY = np.load('/content/drive/MyDrive/data/COUGHVID/cough_y_features_np.npy')\n",
        "\n",
        "virufydatasetX = np.load('/content/drive/MyDrive/data/Virufy/cough_X_features_np.npy')\n",
        "virufydatasetY = np.load('/content/drive/MyDrive/data/Virufy/cough_y_features_np.npy')\n",
        "\n",
        "virufyNoCoCoDadatasetX = np.load('/content/drive/MyDrive/data/NOCOCODA and Virufy/cough_X_features_np.npy')\n",
        "virufyNoCoCoDadatasetY = np.load('/content/drive/MyDrive/data/NOCOCODA and Virufy/cough_y_features_np.npy')\n",
        "\n",
        "\n",
        "print(f\"cambridgeAsymptomaticdatasetX shape : {cambridgeAsymptomaticdatasetX.shape}\")\n",
        "print(f\"cambridgeAsymptomaticdatasetY shape : {cambridgeAsymptomaticdatasetY.shape}\")\n",
        "\n",
        "print(f\"cambridgeSymptomaticdatasetX shape : {cambridgeSymptomaticdatasetX.shape}\")\n",
        "print(f\"cambridgeSymptomaticdatasetY shape : {cambridgeSymptomaticdatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"coswaradatasetX shape : {coswaradatasetX.shape}\")\n",
        "print(f\"coswaradatasetY shape : {coswaradatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"coughviddatasetX shape : {coughviddatasetX.shape}\")\n",
        "print(f\"coughviddatasetY shape : {coughviddatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"virufydatasetX shape : {virufydatasetX.shape}\")\n",
        "print(f\"virufydatasetY shape : {virufydatasetY.shape}\")\n",
        "\n",
        "\n",
        "print(f\"virufyNoCoCoDadatasetX shape : {virufyNoCoCoDadatasetX.shape}\")\n",
        "print(f\"virufyNoCoCoDadatasetY shape : {virufyNoCoCoDadatasetY.shape}\")\n",
        "\n",
        "\n",
        "all_data_cambridgeAsymptomatic = []\n",
        "all_data_cambridgeSymptomatic = []\n",
        "all_data_coswara = []\n",
        "all_data_coughvid = []\n",
        "all_data_virufy = []\n",
        "all_data_virufyNoCoCoDa = []\n",
        "\n",
        "X_data_ls = []\n",
        "Y_data_ls = []\n",
        "dataset_name_ls = []\n",
        "\n",
        "cambridgeAsymptomaticX = []\n",
        "cambridgeSymptomaticX = []\n",
        "coswaraX = []\n",
        "coughvidX = []\n",
        "virufyX = []\n",
        "virufyNoCoCoDaX = []\n",
        "\n",
        "cambridgeAsymptomaticY = []\n",
        "cambridgeSymptomaticY = []\n",
        "coswaraY = []\n",
        "coughvidY = []\n",
        "virufyY = []\n",
        "virufyNoCoCoDaY = []\n",
        "\n",
        "# cross_sectional_study (For cross-sectional studies, refer to the comments in the combined section below)\n",
        "\"\"\"\n",
        "dataset = [\"cambridgeAsymptomaticdataset\",\"cambridgeSymptomaticdataset\",\"coswaradataset\",\"coughviddataset\",\"virufydataset\",\"virufyNoCoCoDadataset\"]\n",
        "datasetX = [cambridgeAsymptomaticdatasetX,cambridgeSymptomaticdatasetX,coswaradatasetX,coughviddatasetX,virufydatasetX,virufyNoCoCoDadatasetX]\n",
        "datasetY = [cambridgeAsymptomaticdatasetY,cambridgeSymptomaticdatasetY,coswaradatasetY,coughviddatasetY,virufydatasetY,virufyNoCoCoDadatasetY]\n",
        "all_data = [all_data_cambridgeAsymptomatic,all_data_cambridgeSymptomatic,all_data_coswara,all_data_coughvid,all_data_virufy,all_data_virufyNoCoCoDa]\n",
        "dataX = [cambridgeAsymptomaticX,cambridgeSymptomaticX,coswaraX,coughvidX,virufyX,virufyNoCoCoDaX]\n",
        "dataY = [cambridgeAsymptomaticY,cambridgeSymptomaticY,coswaraY,coughvidY,virufyY,virufyNoCoCoDaY]\n",
        "\"\"\"\n",
        "\n",
        "#combined_dataset (For the combined dataset, refer to the comments above the cross-sectional study section)\n",
        "\n",
        "dataset = [\"cambridgeAsymptomaticdataset\",\"cambridgeSymptomaticdataset\",\"coswaradataset\",\"coughviddataset\",\"virufyNoCoCoDadataset\"]\n",
        "datasetX = [cambridgeAsymptomaticdatasetX,cambridgeSymptomaticdatasetX,coswaradatasetX,coughviddatasetX,virufyNoCoCoDadatasetX]\n",
        "datasetY = [cambridgeAsymptomaticdatasetY,cambridgeSymptomaticdatasetY,coswaradatasetY,coughviddatasetY,virufyNoCoCoDadatasetY]\n",
        "all_data = [all_data_cambridgeAsymptomatic,all_data_cambridgeSymptomatic,all_data_coswara,all_data_coughvid,all_data_virufyNoCoCoDa]\n",
        "dataX = [cambridgeAsymptomaticX,cambridgeSymptomaticX,coswaraX,coughvidX,virufyNoCoCoDaX]\n",
        "dataY = [cambridgeAsymptomaticY,cambridgeSymptomaticY,coswaraY,coughvidY,virufyNoCoCoDaY]\n",
        "\n",
        "\n",
        "print('')\n",
        "\n",
        "data_all_ls = []\n",
        "\n",
        "num_dataset = len(datasetX)\n",
        "\n",
        "for k in range(num_dataset):\n",
        "    rows,columns = datasetX[k].shape\n",
        "    for i in range(rows):\n",
        "      arr = datasetX[k][i]\n",
        "      col = []\n",
        "      col2 = []\n",
        "      for j in range(1,len(arr)+1,1):\n",
        "         col.append(float(arr[j-1]))\n",
        "         col2.append(float(arr[j-1]))\n",
        "      X_data_ls.append(col2)\n",
        "      dataX[k].append(col2)\n",
        "      if(datasetY[k][i]=='C'):\n",
        "         Y_data_ls.append(0)\n",
        "         col.append(0)\n",
        "         dataY[k].append(0)\n",
        "      else:\n",
        "         Y_data_ls.append(1)\n",
        "         col.append(1)\n",
        "         dataY[k].append(1)\n",
        "      data_all_ls.append(col)\n",
        "      all_data[k].append(col)\n",
        "\n",
        "\n",
        "\n",
        "X_data = np.array(X_data_ls)\n",
        "Y_data = np.array(Y_data_ls)\n",
        "data_all = np.array(data_all_ls)\n",
        "\n",
        "print(f\"X_data: {X_data.shape}\")\n",
        "print(f\"Y_data: {Y_data.shape}\")\n",
        "print(f\"data_all: {data_all.shape}\")\n",
        "\n",
        "print('')\n",
        "\n",
        "for k in range(num_dataset):\n",
        "    print(f\"all_data: {np.array(all_data[k]).shape}\")\n",
        "\n",
        "for k in range(num_dataset):\n",
        "    print(f\"datasetX: {np.array(dataX[k]).shape}\")\n",
        "\n",
        "for k in range(num_dataset):\n",
        "    print(f\"datasetY: {np.array(dataY[k]).shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz-LC_CFNW1N"
      },
      "outputs": [],
      "source": [
        "rows,columns = X_data.shape\n",
        "\n",
        "TARGET_FEATURE_NAME = str(columns+1)\n",
        "#Derive the necessary variables\n",
        "#No. of classes, assuming last column is dependent variable\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "#print([x for x in range(5)])\n",
        "#List of all the columns\n",
        "CSV_HEADER = []\n",
        "for i in range(columns+1):\n",
        "    CSV_HEADER.append(str(i+1))\n",
        "\n",
        "# A list of the numerical feature names.\n",
        "#List of only independent variables\n",
        "FEATURE_NAMES = []\n",
        "\n",
        "for i in range(columns):\n",
        "    FEATURE_NAMES.append(str(i+1))\n",
        "\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in CSV_HEADER  else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trAyW6TqNjvy",
        "outputId": "1f65dce2-5057-429e-d24a-798087ec0b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_data: (3398, 193)\n",
            "Y_data: (3398,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X_data: {X_data.shape}\")\n",
        "print(f\"Y_data: {Y_data.shape}\")\n",
        "\n",
        "\"\"\"\n",
        "# You can try hyperparameter tuning with Bayesian Optimization to gain hands-on experience.\n",
        "\n",
        "# installing library for Bayesian optimization\n",
        "!pip install hyperopt\n",
        "!pip install scikeras\n",
        "\n",
        "X = X_data\n",
        "y = Y_data\n",
        "\n",
        "##############################################################\n",
        "# Bayesian hyperparameter optimization\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, anneal\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "#Random Forest (Bagging of multiple Decision Trees)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "ET = ExtraTreesClassifier()\n",
        "\n",
        "batch_sz_list = [8,16,32,64,128,256]\n",
        "\n",
        "# Defining the hyper parameter space as a dictionary\n",
        "parameter_space = { 'num_trees': hp.quniform('num_trees',5,80,1),\n",
        "                    'depth': hp.quniform('depth',5,30,1),\n",
        "                    'used_features_rate': hp.quniform('used_features_rate',0.5,1,.1),\n",
        "                    'batch_size': hp.choice('batch_size',batch_sz_list),\n",
        "                    'num_epochs': hp.quniform('num_epochs', 10,50,1)\n",
        "                  }\n",
        "\n",
        "\n",
        "# Defining a cost function which the Bayesian algorithm will optimize\n",
        "def objective(parameter_space):\n",
        "\n",
        "    # The accuracy parameter is the average accuracy obtained by cross validation of the data\n",
        "    # See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "    Error = cross_val_score(ET, X, y, cv = 10, scoring='accuracy').mean()\n",
        "\n",
        "    # We return the loss which will be minimized by the fmin() function\n",
        "    return {'loss': -Error, 'status': STATUS_OK }\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Finding out which set of hyperparameters give highest accuracy\n",
        "trials = Trials()\n",
        "best_params = fmin(fn= objective,\n",
        "            space= parameter_space,\n",
        "            #algo= tpe.suggest,\n",
        "            algo=anneal.suggest,  # the logic which chooses next parameter to try\n",
        "            max_evals = 100,\n",
        "            trials= trials)\n",
        "#The best hyperparameters are returned by the function ‘fmin()’.\n",
        "#We have stored the results in the ‘best_params’ variable.\n",
        "print('The best parameters are:', best_params)\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_trees = int(best_params['num_trees'])\n",
        "depth = int(best_params['depth'])\n",
        "used_features_rate = round(best_params['used_features_rate'],1)\n",
        "batch_size = int(batch_sz_list[best_params['batch_size']])\n",
        "num_epochs = int(best_params['num_epochs'])\n",
        "\n",
        "print(f\"num_trees : {num_trees} depth : {depth}  used_features_rate : {used_features_rate}\")\n",
        "print(f\"learning_rate : {learning_rate}  batch_size : {batch_size}  num_epochs : {num_epochs}\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# To save time on Bayesian Optimization, use the hyperparameters below, which have already been calculated for optimal performance.\n",
        "#all_data_eT\n",
        "num_trees = 42\n",
        "depth = 13\n",
        "used_features_rate = 0.7\n",
        "learning_rate = 0.01\n",
        "batch_size = 256\n",
        "num_epochs = 40\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "num_trees = 42\n",
        "depth = 13\n",
        "used_features_rate = 0.7\n",
        "learning_rate = 0.01\n",
        "batch_size = 256\n",
        "num_epochs = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g46jH6xVNvO2"
      },
      "outputs": [],
      "source": [
        "#Import the data from csv with all the relevant columns\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128, num_epochs=1):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=num_epochs,\n",
        "        #num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(lambda features, target: (features, target))\n",
        "    return dataset.cache()\n",
        "\n",
        "# Create one input layer for each feature\n",
        "def create_model_inputs(FEATURE_NAMES):\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = layers.Input(\n",
        "            name=feature_name, shape=(), dtype=tf.float32)\n",
        "    return inputs\n",
        "\n",
        "# If there is any categorical feature, it can be encoded\n",
        "# Since we had only numerical feature, we will skip this step\n",
        "# We will also exdpand the dimension of the feature inputs\n",
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        encoded_feature = inputs[feature_name]\n",
        "        if inputs[feature_name].shape[-1] is None:\n",
        "            encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Am4jQjVkN0nD"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionTree, self).__init__()\n",
        "        self.depth = depth   # Pre-defined depth\n",
        "        self.num_leaves = 2 ** depth  # No of leaves in the tree\n",
        "        self.num_classes = num_classes  # No of classes in the dependent variable\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        # Number of features to be selected for each tree\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        # Select \"num_used_features\" features from the total features\n",
        "        one_hot = np.eye(num_features)\n",
        "        \"\"\"\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        \"\"\"\n",
        "        try:\n",
        "             sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=False)\n",
        "        except ValueError:\n",
        "             sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=True)\n",
        "\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )\n",
        "        # Initiate mu, the probablity of a sample reaching a leaf node\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        # Update probabilities in each level and node.\n",
        "        # Calculate total final output probability\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X-ajnsMqN_WI"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionForest, self).__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D2XQolbZOEkL"
      },
      "outputs": [],
      "source": [
        "def create_tree_model():\n",
        "    inputs = create_model_inputs(FEATURE_NAMES)\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8HesANoFONVx"
      },
      "outputs": [],
      "source": [
        "# Create the forest model by taking input, output and the model structure\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs(FEATURE_NAMES)\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35E6dU0sLJcp"
      },
      "source": [
        "combined_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QeiRzTQqLI9Y"
      },
      "outputs": [],
      "source": [
        "# If using the combined dataset, uncomment the section below and refer to comments in the following cross-sectional study part.\n",
        "\n",
        "from numpy import arange\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "\n",
        "# define thresholds\n",
        "thresholds = arange(0, 1, 0.001)\n",
        "\n",
        "# apply threshold to positive probabilities to create labels\n",
        "def to_labels(pos_probs, threshold):\n",
        "\treturn (pos_probs >= threshold).astype('int')\n",
        "\n",
        "# Now compile the model, train it on train sample and predict it for test sample.\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "\n",
        "    lst_roc_auc = []\n",
        "    lst_precision = []\n",
        "    lst_recall = []\n",
        "    lst_f1 = []\n",
        "    lst_acc = []\n",
        "    lst_speci = []\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    #for i in range(10):\n",
        "    for i, (train_index, test_index) in enumerate(skf.split(X_data, Y_data)):\n",
        "        #all_data_train_fold, all_data_test_fold = data_all[train_index[i]], data_all[test_index[i]]\n",
        "        #print(train_index, test_index)\n",
        "        X_train, X_test = X_data[train_index], X_data[test_index]\n",
        "        y_train, y_test = Y_data[train_index], Y_data[test_index]\n",
        "\n",
        "        print('Before Oversampling')\n",
        "        print(f'X_train shape: {X_train.shape}')\n",
        "        print(f'y_train shape: {y_train.shape}')\n",
        "        counter=Counter(y_train)\n",
        "        print(counter)\n",
        "        oversample = SVMSMOTE()\n",
        "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "        print('After Oversampling')\n",
        "        print(f'X_train shape: {X_train.shape}')\n",
        "        print(f'y_train shape: {y_train.shape}')\n",
        "        counter = Counter(y_train)\n",
        "        print(counter)\n",
        "\n",
        "        all_data_train_fold = []\n",
        "        rows,columns = X_train.shape\n",
        "\n",
        "        for i in range(rows):\n",
        "           col2 = []\n",
        "           arr = X_train[i]\n",
        "           for j in range(1,len(arr)+1,1):\n",
        "              col2.append(float(arr[j-1]))\n",
        "           col2.append(y_train[i])\n",
        "           all_data_train_fold.append(col2);\n",
        "\n",
        "        print(f'all_data_train_fold shape: {np.array(all_data_train_fold).shape}')\n",
        "\n",
        "        all_data_test_fold = []\n",
        "        rows,columns = X_test.shape\n",
        "\n",
        "        for i in range(rows):\n",
        "           col2 = []\n",
        "           arr = X_test[i]\n",
        "           for j in range(1,len(arr)+1,1):\n",
        "              col2.append(float(arr[j-1]))\n",
        "           col2.append(y_test[i])\n",
        "           all_data_test_fold.append(col2);\n",
        "\n",
        "        print(f'all_data_test_fold shape: {np.array(all_data_test_fold).shape}')\n",
        "\n",
        "        # convert array into dataframe\n",
        "        DF = pd.DataFrame(all_data_train_fold)\n",
        "        # save the dataframe as a csv file\n",
        "        DF.to_csv(train_data_file, index=False, header=False)\n",
        "        train_data = get_dataset_from_csv(\n",
        "           train_data_file, batch_size=batch_size, num_epochs=num_epochs\n",
        "        )\n",
        "        model.fit(train_data)\n",
        "\n",
        "        # convert array into dataframe\n",
        "        DF1 = pd.DataFrame(all_data_test_fold)\n",
        "        # save the dataframe as a csv file\n",
        "        DF1.to_csv(test_data_file, index=False, header=False)\n",
        "        test_data = get_dataset_from_csv(\n",
        "           test_data_file, batch_size=batch_size\n",
        "        )\n",
        "        #_, accuracy = model.evaluate(test_data)\n",
        "        #lst_accu_stratified.append(round(accuracy, 2))\n",
        "\n",
        "        #print(f\"y_test: {y_test}\")\n",
        "        true_categories = tf.concat([y for x, y in test_data], axis=0)\n",
        "        #print(f\"true_categories: {true_categories}\")\n",
        "\n",
        "        # keep probabilities for the positive outcome only\n",
        "        Y_pred = model.predict(test_data)\n",
        "        probs = Y_pred[:, 1]\n",
        "        #print(f\"probs: {probs}\")\n",
        "\n",
        "        # evaluate each threshold\n",
        "        #scores = [precision_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        #scores = [f1_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "        # get best threshold\n",
        "        ix = argmax(scores)\n",
        "        #print(f\"Optimal threshold score: {ix}\")\n",
        "        #print(f\"Thresholds[ix]: {thresholds[ix]}\")\n",
        "\n",
        "        #Y_pred = model.predict(test_data)\n",
        "        #probs = Y_pred[:, 1]\n",
        "        #print(probs)\n",
        "        #print(f\"probsLast: {probs}\")\n",
        "        #print(f\"to_labels(probs, thresholds[ix]): {to_labels(probs[len(probs)-1], thresholds[ix])}\")\n",
        "\n",
        "        conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))\n",
        "        print(conf_matrix)\n",
        "\n",
        "        tp, fn, fp, tn = conf_matrix.ravel()\n",
        "        print(f\"True positive: {tp}\")\n",
        "        print(f\"True negative: {tn}\")\n",
        "        print(f\"False positive: {fp}\")\n",
        "        print(f\"False negative: {fn}\")\n",
        "\n",
        "        lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));\n",
        "        lst_acc.append((tp+tn)/(tp+tn+fp+fn));\n",
        "\n",
        "        if(tn+fp == 0):\n",
        "           lst_speci.append(0)\n",
        "        else:\n",
        "           lst_speci.append(tn/(tn+fp));\n",
        "\n",
        "        if(tp+fp == 0):\n",
        "           precision = 0\n",
        "           lst_precision.append(precision)\n",
        "        else:\n",
        "           precision = tp/(tp+fp)\n",
        "           lst_precision.append(precision);\n",
        "\n",
        "        if(tp+fn == 0):\n",
        "           recall = 0\n",
        "           lst_recall.append(recall)\n",
        "        else:\n",
        "           recall = tp/(tp+fn)\n",
        "           lst_recall.append(recall);\n",
        "\n",
        "        if(precision+recall == 0):\n",
        "           lst_f1.append(0)\n",
        "        else:\n",
        "           lst_f1.append((2*precision*recall)/(precision+recall));\n",
        "\n",
        "        print(f\"accuracy : {(tp+tn)/(tp+tn+fp+fn)}\")\n",
        "        print(f\"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}\")\n",
        "        print(f\"precision : {precision}\")\n",
        "        print(f\"recall : {recall}\")\n",
        "        print(f\"f1_score : {(2*precision*recall)/(precision+recall)}\")\n",
        "        print(f\"specificity : {tn/(tn+fp)}\")\n",
        "\n",
        "        print('')\n",
        "\n",
        "\n",
        "    print(f\"mean_of_acc : {mean(lst_acc)}\")\n",
        "    print(f\"mean_of_roc_auc_score : {mean(lst_roc_auc)}\")\n",
        "    print(f\"mean_of_precision_score : {mean(lst_precision)}\")\n",
        "    print(f\"mean_of_recall_score : {mean(lst_recall)}\")\n",
        "    print(f\"mean_of_f1_score : {mean(lst_f1)}\")\n",
        "    print(f\"mean_of_specificity_score : {mean(lst_speci)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT1df9brC2Ox"
      },
      "source": [
        "cross_sectional_study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "PC6XKX1wOQwU",
        "outputId": "f7ce4d71-78a9-4086-ac7a-7de54794489f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom numpy import arange\\nfrom numpy import argmax\\nfrom sklearn.metrics import f1_score\\nfrom collections import Counter\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.over_sampling import SVMSMOTE\\n\\n# define thresholds\\nthresholds = arange(0, 1, 0.001)\\n\\n# apply threshold to positive probabilities to create labels\\ndef to_labels(pos_probs, threshold):\\n\\treturn (pos_probs >= threshold).astype(\\'int\\')\\n\\n# Now compile the model, train it on train sample and predict it for test sample.\\ndef run_experiment(model):\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\\n        loss=keras.losses.SparseCategoricalCrossentropy(),\\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\\n    )\\n\\n    # Feature Scaling for input features.\\n    #scaler = preprocessing.MinMaxScaler()\\n\\n    # Create StratifiedKFold object.\\n    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\\n    lst_roc_auc = []\\n    lst_precision = []\\n    lst_recall = []\\n    lst_f1 = []\\n    lst_acc = []\\n    lst_speci = []\\n\\n    X_train = dataX[5]\\n    y_train = dataY[5]\\n\\n    X_train = np.array(X_train)\\n    y_train = np.array(y_train)\\n\\n    print(\\'Before Oversampling\\')\\n    print(f\\'X_train shape: {X_train.shape}\\')\\n    print(f\\'y_train shape: {y_train.shape}\\')\\n    counter=Counter(y_train)\\n    print(counter)\\n    oversample = SVMSMOTE()\\n    X_train, y_train = oversample.fit_resample(X_train, y_train)\\n    print(\\'After Oversampling\\')\\n    print(f\\'X_train shape: {X_train.shape}\\')\\n    print(f\\'y_train shape: {y_train.shape}\\')\\n    counter = Counter(y_train)\\n    print(counter)\\n\\n    all_data_train_fold = []\\n    rows,columns = X_train.shape\\n\\n    for i in range(rows):\\n        col2 = []\\n        arr = X_train[i]\\n        for j in range(1,len(arr)+1,1):\\n          col2.append(float(arr[j-1]))\\n        col2.append(y_train[i])\\n        all_data_train_fold.append(col2);\\n\\n    print(f\\'all_data_train_fold 4hape: {np.array(all_data_train_fold).shape}\\')\\n    print(f\"Trained by {dataset[5]}:\")\\n    # convert array into dataframe\\n    DF = pd.DataFrame(all_data_train_fold)\\n    # save the dataframe as a csv file\\n    DF.to_csv(train_data_file, index=False, header=False)\\n    train_data = get_dataset_from_csv(\\n       train_data_file, batch_size=batch_size, num_epochs=num_epochs\\n    )\\n    model.fit(train_data)\\n    for j in range(6):\\n      if(j!=5):\\n         print(f\"Tested by {dataset[j]}:\")\\n         # convert array into dataframe\\n         DF1 = pd.DataFrame(all_data[j])\\n         # save the dataframe as a csv file\\n         DF1.to_csv(test_data_file, index=False, header=False)\\n         test_data = get_dataset_from_csv(\\n            test_data_file, batch_size=batch_size\\n         )\\n\\n         true_categories = tf.concat([y for x, y in test_data], axis=0)\\n         #print(true_categories)\\n\\n         # keep probabilities for the positive outcome only\\n         Y_pred = model.predict(test_data)\\n         probs = Y_pred[:, 1]\\n         #print(f\"probs: {probs}\")\\n\\n         scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]\\n         ix = argmax(scores)\\n         #print(f\"Optimal threshold score: {ix}\")\\n         print(f\"Thresholds[ix]: {thresholds[ix]}\")\\n\\n         conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))\\n         print(conf_matrix)\\n\\n         tp, fn, fp, tn = conf_matrix.ravel()\\n         print(f\"True positive: {tp}\")\\n         print(f\"True negative: {tn}\")\\n         print(f\"False positive: {fp}\")\\n         print(f\"False negative: {fn}\")\\n\\n         lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));\\n         lst_acc.append((tp+tn)/(tp+tn+fp+fn));\\n\\n         if(tn+fp == 0):\\n            lst_speci.append(0)\\n         else:\\n            lst_speci.append(tn/(tn+fp));\\n\\n         if(tp+fp == 0):\\n            precision = 0\\n            lst_precision.append(precision)\\n         else:\\n            precision = tp/(tp+fp)\\n            lst_precision.append(precision);\\n\\n         if(tp+fn == 0):\\n            recall = 0\\n            lst_recall.append(recall)\\n         else:\\n            recall = tp/(tp+fn)\\n            lst_recall.append(recall);\\n\\n         if(precision+recall == 0):\\n            lst_f1.append(0)\\n         else:\\n            lst_f1.append((2*precision*recall)/(precision+recall));\\n\\n         print(f\"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}\")\\n         print(f\"precision : {precision}\")\\n         print(f\"recall : {recall}\")\\n         print(f\"f1_score : {(2*precision*recall)/(precision+recall)}\")\\n         print(\\'\\')\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If you perform cross sectional study, uncomment the section below and refer to comments in the above combined dataset part.\n",
        "\"\"\"\n",
        "from numpy import arange\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "\n",
        "# define thresholds\n",
        "thresholds = arange(0, 1, 0.001)\n",
        "\n",
        "# apply threshold to positive probabilities to create labels\n",
        "def to_labels(pos_probs, threshold):\n",
        "\treturn (pos_probs >= threshold).astype('int')\n",
        "\n",
        "# Now compile the model, train it on train sample and predict it for test sample.\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    # Feature Scaling for input features.\n",
        "    #scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "    # Create StratifiedKFold object.\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    lst_roc_auc = []\n",
        "    lst_precision = []\n",
        "    lst_recall = []\n",
        "    lst_f1 = []\n",
        "    lst_acc = []\n",
        "    lst_speci = []\n",
        "\n",
        "    X_train = dataX[4] # For the cross-sectional study, indicate that the model was trained on the 5th dataset. Change this value to train on other datasets.\n",
        "    y_train = dataY[4] # For the cross-sectional study, indicate that the model was trained on the 5th dataset. Change this value to train on other datasets.\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    print('Before Oversampling')\n",
        "    print(f'X_train shape: {X_train.shape}')\n",
        "    print(f'y_train shape: {y_train.shape}')\n",
        "    counter=Counter(y_train)\n",
        "    print(counter)\n",
        "    oversample = SVMSMOTE()\n",
        "    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "    print('After Oversampling')\n",
        "    print(f'X_train shape: {X_train.shape}')\n",
        "    print(f'y_train shape: {y_train.shape}')\n",
        "    counter = Counter(y_train)\n",
        "    print(counter)\n",
        "\n",
        "    all_data_train_fold = []\n",
        "    rows,columns = X_train.shape\n",
        "\n",
        "    for i in range(rows):\n",
        "        col2 = []\n",
        "        arr = X_train[i]\n",
        "        for j in range(1,len(arr)+1,1):\n",
        "          col2.append(float(arr[j-1]))\n",
        "        col2.append(y_train[i])\n",
        "        all_data_train_fold.append(col2);\n",
        "\n",
        "    print(f'all_data_train_fold 4hape: {np.array(all_data_train_fold).shape}')\n",
        "    print(f\"Trained by {dataset[5]}:\")\n",
        "    # convert array into dataframe\n",
        "    DF = pd.DataFrame(all_data_train_fold)\n",
        "    # save the dataframe as a csv file\n",
        "    DF.to_csv(train_data_file, index=False, header=False)\n",
        "    train_data = get_dataset_from_csv(\n",
        "       train_data_file, batch_size=batch_size, num_epochs=num_epochs\n",
        "    )\n",
        "    model.fit(train_data)\n",
        "    for j in range(6):\n",
        "      if(j!=4):           # Indicates testing of the model on all datasets except dataset 5th.\n",
        "         print(f\"Tested by {dataset[j]}:\")\n",
        "         # convert array into dataframe\n",
        "         DF1 = pd.DataFrame(all_data[j])\n",
        "         # save the dataframe as a csv file\n",
        "         DF1.to_csv(test_data_file, index=False, header=False)\n",
        "         test_data = get_dataset_from_csv(\n",
        "            test_data_file, batch_size=batch_size\n",
        "         )\n",
        "\n",
        "         true_categories = tf.concat([y for x, y in test_data], axis=0)\n",
        "         #print(true_categories)\n",
        "\n",
        "         # keep probabilities for the positive outcome only\n",
        "         Y_pred = model.predict(test_data)\n",
        "         probs = Y_pred[:, 1]\n",
        "         #print(f\"probs: {probs}\")\n",
        "\n",
        "         scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]\n",
        "         ix = argmax(scores)\n",
        "         #print(f\"Optimal threshold score: {ix}\")\n",
        "         print(f\"Thresholds[ix]: {thresholds[ix]}\")\n",
        "\n",
        "         conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))\n",
        "         print(conf_matrix)\n",
        "\n",
        "         tp, fn, fp, tn = conf_matrix.ravel()\n",
        "         print(f\"True positive: {tp}\")\n",
        "         print(f\"True negative: {tn}\")\n",
        "         print(f\"False positive: {fp}\")\n",
        "         print(f\"False negative: {fn}\")\n",
        "\n",
        "         lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));\n",
        "         lst_acc.append((tp+tn)/(tp+tn+fp+fn));\n",
        "\n",
        "         if(tn+fp == 0):\n",
        "            lst_speci.append(0)\n",
        "         else:\n",
        "            lst_speci.append(tn/(tn+fp));\n",
        "\n",
        "         if(tp+fp == 0):\n",
        "            precision = 0\n",
        "            lst_precision.append(precision)\n",
        "         else:\n",
        "            precision = tp/(tp+fp)\n",
        "            lst_precision.append(precision);\n",
        "\n",
        "         if(tp+fn == 0):\n",
        "            recall = 0\n",
        "            lst_recall.append(recall)\n",
        "         else:\n",
        "            recall = tp/(tp+fn)\n",
        "            lst_recall.append(recall);\n",
        "\n",
        "         if(precision+recall == 0):\n",
        "            lst_f1.append(0)\n",
        "         else:\n",
        "            lst_f1.append((2*precision*recall)/(precision+recall));\n",
        "\n",
        "         print(f\"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}\")\n",
        "         print(f\"precision : {precision}\")\n",
        "         print(f\"recall : {recall}\")\n",
        "         print(f\"f1_score : {(2*precision*recall)/(precision+recall)}\")\n",
        "         print('')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB4iN4IHOVs0",
        "outputId": "68db2eba-57c1-4422-fd17-981d1e4e8258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1996, np.int64(0): 1062})\n",
            "After Oversampling\n",
            "X_train shape: (3992, 193)\n",
            "y_train shape: (3992,)\n",
            "Counter({np.int64(0): 1996, np.int64(1): 1996})\n",
            "all_data_train_fold shape: (3992, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - loss: 0.6470 - sparse_categorical_accuracy: 0.7766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4s/step\n",
            "[[ 68  51]\n",
            " [ 53 168]]\n",
            "True positive: 68\n",
            "True negative: 168\n",
            "False positive: 53\n",
            "False negative: 51\n",
            "accuracy : 0.6941176470588235\n",
            "roc_auc_score : 0.6658047834518422\n",
            "precision : 0.5619834710743802\n",
            "recall : 0.5714285714285714\n",
            "f1_score : 0.5666666666666667\n",
            "specificity : 0.7601809954751131\n",
            "\n",
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1995, np.int64(0): 1063})\n",
            "After Oversampling\n",
            "X_train shape: (3990, 193)\n",
            "y_train shape: (3990,)\n",
            "Counter({np.int64(0): 1995, np.int64(1): 1995})\n",
            "all_data_train_fold shape: (3990, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - loss: 0.5995 - sparse_categorical_accuracy: 0.9766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "[[102  16]\n",
            " [ 21 201]]\n",
            "True positive: 102\n",
            "True negative: 201\n",
            "False positive: 21\n",
            "False negative: 16\n",
            "accuracy : 0.8911764705882353\n",
            "roc_auc_score : 0.8849060925332112\n",
            "precision : 0.8292682926829268\n",
            "recall : 0.864406779661017\n",
            "f1_score : 0.8464730290456431\n",
            "specificity : 0.9054054054054054\n",
            "\n",
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1995, np.int64(0): 1063})\n",
            "After Oversampling\n",
            "X_train shape: (3990, 193)\n",
            "y_train shape: (3990,)\n",
            "Counter({np.int64(0): 1995, np.int64(1): 1995})\n",
            "all_data_train_fold shape: (3990, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - loss: 0.5951 - sparse_categorical_accuracy: 0.9861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "[[109   9]\n",
            " [ 14 208]]\n",
            "True positive: 109\n",
            "True negative: 208\n",
            "False positive: 14\n",
            "False negative: 9\n",
            "accuracy : 0.9323529411764706\n",
            "roc_auc_score : 0.9303328752481295\n",
            "precision : 0.8861788617886179\n",
            "recall : 0.923728813559322\n",
            "f1_score : 0.9045643153526971\n",
            "specificity : 0.9369369369369369\n",
            "\n",
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1995, np.int64(0): 1063})\n",
            "After Oversampling\n",
            "X_train shape: (3990, 193)\n",
            "y_train shape: (3990,)\n",
            "Counter({np.int64(0): 1995, np.int64(1): 1995})\n",
            "all_data_train_fold shape: (3990, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - loss: 0.5937 - sparse_categorical_accuracy: 0.9863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "[[111   7]\n",
            " [ 19 203]]\n",
            "True positive: 111\n",
            "True negative: 203\n",
            "False positive: 19\n",
            "False negative: 7\n",
            "accuracy : 0.9235294117647059\n",
            "roc_auc_score : 0.9275461902580547\n",
            "precision : 0.8538461538461538\n",
            "recall : 0.940677966101695\n",
            "f1_score : 0.8951612903225806\n",
            "specificity : 0.9144144144144144\n",
            "\n",
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1995, np.int64(0): 1063})\n",
            "After Oversampling\n",
            "X_train shape: (3990, 193)\n",
            "y_train shape: (3990,)\n",
            "Counter({np.int64(0): 1995, np.int64(1): 1995})\n",
            "all_data_train_fold shape: (3990, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - loss: 0.5926 - sparse_categorical_accuracy: 0.9874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "[[111   7]\n",
            " [  7 215]]\n",
            "True positive: 111\n",
            "True negative: 215\n",
            "False positive: 7\n",
            "False negative: 7\n",
            "accuracy : 0.9588235294117647\n",
            "roc_auc_score : 0.9545732172850818\n",
            "precision : 0.940677966101695\n",
            "recall : 0.940677966101695\n",
            "f1_score : 0.940677966101695\n",
            "specificity : 0.9684684684684685\n",
            "\n",
            "Before Oversampling\n",
            "X_train shape: (3058, 193)\n",
            "y_train shape: (3058,)\n",
            "Counter({np.int64(1): 1995, np.int64(0): 1063})\n",
            "After Oversampling\n",
            "X_train shape: (3990, 193)\n",
            "y_train shape: (3990,)\n",
            "Counter({np.int64(0): 1995, np.int64(1): 1995})\n",
            "all_data_train_fold shape: (3990, 194)\n",
            "all_data_test_fold shape: (340, 194)\n",
            "    232/Unknown \u001b[1m11s\u001b[0m 47ms/step - loss: 0.5923 - sparse_categorical_accuracy: 0.9864"
          ]
        }
      ],
      "source": [
        "tree_model = create_tree_model()\n",
        "run_experiment(tree_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzlZHK5fT1TI"
      },
      "outputs": [],
      "source": [
        "forest_model = create_forest_model()\n",
        "run_experiment(forest_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
