import numpy as np
import tensorflow as tf
import pandas as pd
from pandas import read_csv
from numpy import arange, argmax
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from statistics import mean
from collections import Counter
from imblearn.over_sampling import SVMSMOTE

# define thresholds
thresholds = arange(0, 1, 0.001)

# apply threshold to positive probabilities to create labels
def to_labels(pos_probs, threshold):
	return (pos_probs >= threshold).astype('int')

# Now compile the model, train it on train sample and predict it for test sample.
def run_experiment(model):
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )

    lst_roc_auc = []
    lst_precision = []
    lst_recall = []
    lst_f1 = []
    lst_acc = []
    lst_speci = []

    skf = StratifiedKFold(n_splits=10, shuffle=True)

    for i in range(10):
        X_train, X_test = X_data_fs[train_index[i]], X_data_fs[test_index[i]]
        y_train, y_test = Y_data[train_index[i]], Y_data[test_index[i]]

        print('Before Oversampling')
        print(f'X_train shape: {X_train.shape}')
        print(f'y_train shape: {y_train.shape}')
        counter=Counter(y_train)
        print(counter)
        oversample = SVMSMOTE()
        X_train, y_train = oversample.fit_resample(X_train, y_train)

        print('After Oversampling')
        print(f'X_train shape: {X_train.shape}')
        print(f'y_train shape: {y_train.shape}')
        counter = Counter(y_train)
        print(counter)

        all_data_train_fold = []
        rows,columns = X_train.shape

        for i in range(rows):
           col2 = []
           arr = X_train[i]
           for j in range(1,len(arr)+1,1):
              col2.append(float(arr[j-1]))
           col2.append(y_train[i])
           all_data_train_fold.append(col2);

        print(f'all_data_train_fold shape: {np.array(all_data_train_fold).shape}')

        all_data_test_fold = []
        rows,columns = X_test.shape

        for i in range(rows):
           col2 = []
           arr = X_test[i]
           for j in range(1,len(arr)+1,1):
              col2.append(float(arr[j-1]))
           col2.append(y_test[i])
           all_data_test_fold.append(col2);

        print(f'all_data_test_fold shape: {np.array(all_data_test_fold).shape}')

        # convert array into dataframe
        DF = pd.DataFrame(all_data_train_fold)

        # save the dataframe as a csv file
        DF.to_csv(train_data_file, index=False, header=False)
        train_data = get_dataset_from_csv(
           train_data_file, batch_size=batch_size, num_epochs=num_epochs
        )
        model.fit(train_data)

        # convert array into dataframe
        DF1 = pd.DataFrame(all_data_test_fold)

        # save the dataframe as a csv file
        DF1.to_csv(test_data_file, index=False, header=False)
        test_data = get_dataset_from_csv(
           test_data_file, batch_size=batch_size
        )
        
        true_categories = tf.concat([y for x, y in test_data], axis=0)
        print(f"true_categories: {true_categories}")

        # keep probabilities for the positive outcome only
        Y_pred = model.predict(test_data)
        probs = Y_pred[:, 1]
        
        # evaluate each threshold
        scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]

        # get best threshold
        ix = argmax(scores)
        print(f"TM: {thresholds[ix]}")
        print(f"To Labels(probs, thresholds[ix]): {to_labels(probs, thresholds[ix])}")

        conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))
        print(conf_matrix)

        tp, fn, fp, tn = conf_matrix.ravel()
        print(f"True positive: {tp}")
        print(f"True negative: {tn}")
        print(f"False positive: {fp}")
        print(f"False negative: {fn}")

        lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));

        if(tp+fp == 0):
           lst_precision.append(0)
        else:
           lst_precision.append(tp/(tp+fp));

        if(tp+fn == 0):
           lst_recall.append(0)
        else:
           lst_recall.append(tp/(tp+fn));

        print(f"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}")
        print('')

    print(f"mean_of_roc_auc_score : {mean(lst_roc_auc)}")
    print(f"mean_of_precision : {mean(lst_precision)}")
    print(f"mean_of_recall : {mean(lst_recall)}")