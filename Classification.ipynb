import tensorflow as tf
import pandas as pd
from pandas import read_csv
from numpy import arange, argmax
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from statistics import mean

# define thresholds
thresholds = arange(0, 1, 0.001)

# apply threshold to positive probabilities to create labels
def to_labels(pos_probs, threshold):
	return (pos_probs >= threshold).astype('int')

# Now compile the model, train it on train sample and predict it for test sample.
def run_experiment(model):
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )

    lst_roc_auc = []
    lst_precision = []
    lst_recall = []
    lst_f1 = []
    lst_acc = []
    lst_speci = []

    skf = StratifiedKFold(n_splits=10, shuffle=True)

    for i in range(10):
        all_data_train_fold, all_data_test_fold = all_data[train_index[i]], all_data[test_index[i]]
        
        # convert array into dataframe
        DF = pd.DataFrame(all_data_train_fold)

        # save the dataframe as a csv file
        DF.to_csv(train_data_file, index=False, header=False)
        train_data = get_dataset_from_csv(
           train_data_file, batch_size=batch_size, num_epochs=num_epochs
        )
        model.fit(train_data)

        # convert array into dataframe
        DF1 = pd.DataFrame(all_data_test_fold)

        # save the dataframe as a csv file
        DF1.to_csv(test_data_file, index=False, header=False)
        test_data = get_dataset_from_csv(
           test_data_file, batch_size=batch_size
        )

        true_categories = tf.concat([y for x, y in test_data], axis=0)
        print(true_categories)

        # keep probabilities for the positive outcome only
        Y_pred = model.predict(test_data)

        probs = Y_pred[:, 1]

        # evaluate each threshold
        scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]

        # get best threshold
        ix = argmax(scores)

        conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))
        print(conf_matrix)

        tp, fn, fp, tn = conf_matrix.ravel()
        print(f"True positive: {tp}")
        print(f"True negative: {tn}")
        print(f"False positive: {fp}")
        print(f"False negative: {fn}")

        lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));
        lst_acc.append((tp+tn)/(tp+tn+fp+fn));

        if(tn+fp == 0):
           lst_speci.append(0)
        else:
           lst_speci.append(tn/(tn+fp));

        if(tp+fp == 0):
           precision = 0
           lst_precision.append(precision)
        else:
           precision = tp/(tp+fp)
           lst_precision.append(precision);

        if(tp+fn == 0):
           recall = 0
           lst_recall.append(recall)
        else:
           recall = tp/(tp+fn)
           lst_recall.append(recall);

        if(precision+recall == 0):
           lst_f1.append(0)
        else:
           lst_f1.append((2*precision*recall)/(precision+recall));

        print(f"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}")
        print(f"precision : {precision}")
        print(f"recall : {recall}")
        print(f"accuracy : {(tp+tn)/(tp+tn+fp+fn)}")
        print(f"f1_score : {(2*precision*recall)/(precision+recall)}")
        print(f"specificity : {tn/(tn+fp)}")

        print('')

    print(f"mean_of_accuracy : {mean(lst_acc)}")
    print(f"mean_of_roc_auc_score : {mean(lst_roc_auc)}")
    print(f"mean_of_precision : {mean(lst_precision)}")
    print(f"mean_of_recall : {mean(lst_recall)}")
    print(f"mean_of_f1_score : {mean(lst_f1)}")
    print(f"mean_of_specificity : {mean(lst_speci)}")